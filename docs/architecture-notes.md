Architecture Notes
Merchant Transaction Analytics 
Andrew Jarmin 

1. **How I structured the backend**
The backend I used was solely built in Python using Flask API web framework and several Python libraries to make the application run smooth. I choose Flask API for this application project because I am more familiar with it, it is robust, and easy to use. I organized this API backend into three main sections: api, services, and data. With app.py being the entry point of the application at the root level of the backend directory and additional directories such as (/utils) and (/tests). 

API (api/) handles HTTP concerns only, essentially the "request" traffic controller, this handles the ability to extract the query parameters and direct it to the right functions, returning JSON. There are 4 routes 
- route("/api/transactions"): returns all transactions with optional filtering 
- route("/api/summary/mtd"): show month to date transactions 
- route("/api/summary/monthly"): shows transactions aggregated by month
- route("/api/health"): simple health check for the API

Services (services/) contain the core business logic. The aggregator module (aggregator.py) exposes three functions: filter_transactions for applying filters, mtd_summary for month-to-date aggregation, and month_by_month_summary for historical monthly breakdowns. Having these functions in its own seperate module means it can be extended with additional functionality.

Data (data/) is a static JSON file containing 100 mock transactions. This file is generated by a standalone utility script in (/utils/generator.py) and loaded at runtime by the application entry point in app.py

app.py is the entrypoint into the application and wires everything together, this imports data and registers the api routes making it straightforward to swap in different data sources or functions. 


2. **How I modeled the data**
Each transaction is a JSON object with the following fields (camel case): 
- transactionId (string): A unique identifier in the format "T####-####-#######", generated to resemble real transaction IDs. 
- merchantId (string): Identifies the merchant, "M1" through "M5", kept this simple. 
- amount (float): Transaction amount rounded to two decimal points, example: 61.42
- cardBrand (string): One of four values: "Visa", "Mastercard", "Amex", or "Discover"
- status (string): "Approved" or "Declined" 
- declineReasonCode (string or null): A two digit code "01", "02", or "03" for declined transactions, and null for approved ones. 
    - "01": represents insufficient funds 
    - "02": represents invalid card
    - "03": represents suspected fraud
- transactionDate (string): An ISO 8601 timestamp used for date-based filtering and grouping in the format "YYYY-MM-DDTHH:MM:SS"

I chose JSON over CSV because it maps directly to Python dictionaries and being able to use the library Jsonify makes it easy to work with JSON data sets. I am also very familiar with JSON datasets compared to CSV. 

3. **How I approached filtering and aggregation**

**Filtering**
The filter_transaction function ref:(backend/services/aggregator.py) takes a list of transactions and three optional parameters: cardBrand, status, and declineReasonCode. It loops through the list once and skips any transaction that doesn't match the active filters. If no filters are provided, all transactions pass through. 

**Aggregation**
There are two aggregation endpoints, both of which apply filters before aggregating: 

Month-to-Date (MTD) Summary: 
First it narrows the dataset to only transactions in the current calendar month, then applies any user filters (for ex: "Mastercard", "Approved") and then computes the totals: total transactions, approved count, declined count, dollar amounts, a breakdown by card brand, and a breakdown by decline reason code. 

Month-by-Month Summary: 
First applies filters to the full dataset, then group transactions by their year-month. Each month gets the same set of computed metric as the MTD summary. The results include a sortKey field (formatted as (YYYY-MM)) so the frontend can sort months chronologically without having to parse display labels like "Jan 2025" 

4. **Tradeoffs I made** 
One trade off I made was deciding the web framework to use while building this application, specifically the tradeoff between FlaskAPI and FastAPI. I chose Flask because it's the framework I'm most comfortable with and it let me move quickly within the 3-day window. FastAPI does have automatic request validation, and async support out of the box, but for a small synchronous API with three endpoints, those benefits didn't justify the learning curve tradeoff.

Also, ultimately deciding the backend to be in Python vs Javascript because Python has good libraries like pytest and I work well with python dictionaries 


5. **What I would improve with more time**
Refactor to FastAPI: 
With more time, I'd migrate the backend from Flask to FASTAPI, the main benefit would be using Pyndanic models to automatically validate incoming query parameters. (For example, rejecting an invalid cardBrand value with a clear 400 error instead of silently returning empty results). 

Add error handling + upgrading on the frontend: 
The current fetch calls don't have a try/catch block, so if the backend is unreachable, the dashabord silently fails with no feedback, having a "Unable to connect to server" would be better for testing instead of leaving the page empty. And also moving away from Vanilla HTML/CSS to REACT (component based + faster).

Expand test coverage: 
I would like to add edge case tests for scenarios like malformed date strings, missing fields, and empty datasets. 